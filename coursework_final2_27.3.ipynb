{"nbformat_minor": 1, "cells": [{"source": "#Preparing directory in Local FileSystem \n%cd ~/notebook/work/\n!mkdir BDCW\n%cd ~/notebook/work/BDCW\n# Getting file from UCI repository\n!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00438/Health-News-Tweets.zip\n!unzip -o -q Health-News-Tweets.zip -d ~/notebook/work/BDCW\n# Deleting Unused Folder\n!rm -r __MACOSX\n!rm  Health-News-Tweets.zip\n# Setting it as home directory \n%cd ~/notebook/work/BDCW/Health-Tweets\n#cause pyspark bug, probably strange characters > removing...\n!rm goodhealth.txt\n# Displaying files\n!ls", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "/gpfs/global_fs01/sym_shared/YPProdSpark/user/sb58-fd12fb10398921-2dd0a6f275af/notebook/work\nmkdir: cannot create directory \u2018BDCW\u2019: File exists\n/gpfs/global_fs01/sym_shared/YPProdSpark/user/sb58-fd12fb10398921-2dd0a6f275af/notebook/work/BDCW\n--2018-03-26 05:16:16--  https://archive.ics.uci.edu/ml/machine-learning-databases/00438/Health-News-Tweets.zip\nResolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.249\nConnecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.249|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3452966 (3.3M) [application/zip]\nSaving to: \u2018Health-News-Tweets.zip\u2019\n\n100%[======================================>] 3,452,966   5.48MB/s   in 0.6s   \n\n2018-03-26 05:16:17 (5.48 MB/s) - \u2018Health-News-Tweets.zip\u2019 saved [3452966/3452966]\n\n/gpfs/global_fs01/sym_shared/YPProdSpark/user/sb58-fd12fb10398921-2dd0a6f275af/notebook/work/BDCW/Health-Tweets\nbbchealth.txt\t    foxnewshealth.txt\t  msnhealthnews.txt  reuters_health.txt\ncbchealth.txt\t    gdnhealthcare.txt\t  NBChealth.txt      usnewshealth.txt\ncnnhealth.txt\t    KaiserHealthNews.txt  nprhealth.txt      wsjhealth.txt\neverydayhealth.txt  latimeshealth.txt\t  nytimeshealth.txt\n"}], "execution_count": 54}, {"source": "# Load Spark context \nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 1}, {"source": "from pathlib import Path\nimport re\n\n# Setting default working directory\n%cd ~/notebook/work/BDCW\n# Define function lockandload needed to run through \ndef LockAndLoad(address):\n\n    p = Path(address)\n    dirs = list(p.iterdir())\n\n    print(dirs)\n    rddarray=[]\n    filenames=[]\n\n    for i in dirs:\n        text = sc.textFile(str(i.absolute())).map(lambda a: (a.split('|')))\n        name = sc.wholeTextFiles(str(i.absolute()))\n        name = name.map(lambda x: x[0])\n        filenames.append(name)\n        text.take(10)\n        #fn = fn.map(lambda a: (a.split('|')))\n        rddarray.append(text)\n    \n    print('len(rddarray)',len(rddarray))\n    print('len(filenames)',len(filenames))\n\n    full_rdd = sc.parallelize(str(\"\"))\n        \n    for i in range (0,15):\n        p = str(filenames[i].take(1))\n        nm = re.search('Health-Tweets(.*?).txt', p).group(1)\n        rdd = rddarray[i].map(lambda x: (str(nm),x[2],x[1]))\n        full_rdd = full_rdd.union(rdd)\n\n    return full_rdd\n\n\nfull_rdd = LockAndLoad('Health-Tweets')", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "/gpfs/global_fs01/sym_shared/YPProdSpark/user/sb58-fd12fb10398921-2dd0a6f275af/notebook/work/BDCW\n[PosixPath('Health-Tweets/gdnhealthcare.txt'), PosixPath('Health-Tweets/NBChealth.txt'), PosixPath('Health-Tweets/usnewshealth.txt'), PosixPath('Health-Tweets/KaiserHealthNews.txt'), PosixPath('Health-Tweets/reuters_health.txt'), PosixPath('Health-Tweets/bbchealth.txt'), PosixPath('Health-Tweets/msnhealthnews.txt'), PosixPath('Health-Tweets/foxnewshealth.txt'), PosixPath('Health-Tweets/cbchealth.txt'), PosixPath('Health-Tweets/cnnhealth.txt'), PosixPath('Health-Tweets/latimeshealth.txt'), PosixPath('Health-Tweets/nytimeshealth.txt'), PosixPath('Health-Tweets/nprhealth.txt'), PosixPath('Health-Tweets/wsjhealth.txt'), PosixPath('Health-Tweets/everydayhealth.txt')]\nlen(rddarray) 15\nlen(filenames) 15\n"}], "execution_count": 55}, {"source": "from pyspark.sql import functions as F\nfrom pyspark.sql.functions import concat, col, lit, desc\nimport matplotlib.pyplot as plt\n#Count data by time:\n\ndef countby(rdd, var):\n    # delete any previous pending plots, if any...\n    plt.clf()\n    # transform rdd to dataframe object\n    stg_ssn = full_rdd.map(lambda p: Row(service=p[0], text=p[1], date=p[2]))\n    stg = sqlContext.createDataFrame(full_rdd, ['service','text','date'])\n    # Setting time to legible format:\n    stg = stg.withColumn('date', concat(stg.date.substr(5,3)))\n    # Count by var, it can be service or date (months and years)\n    counts = stg.groupBy([var]).count().alias('counts').sort(desc(\"count\"))\n    # reduced df is transformed into pandas format\n    df = counts.toPandas()\n    # plotting...\n    df.plot(kind='bar',x=var,y='count',colormap='winter_r')\n    plt.show()\n\ncountby(full_rdd, 'date')\ncountby(full_rdd, 'service')", "cell_type": "code", "metadata": {"pixiedust": {"displayParams": {}}}, "outputs": [], "execution_count": 58}, {"source": "stg_ssn = full_rdd.map(lambda p: Row(service=p[0], text=p[1], date=p[2]))\nstg = sqlContext.createDataFrame(full_rdd, ['service','text','date'])\nstg.show()", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------+--------------------+--------------------+\n|       service|                text|                date|\n+--------------+--------------------+--------------------+\n|/gdnhealthcare|Are you a member ...|Wed Apr 08 19:48:...|\n|/gdnhealthcare|What is palliativ...|Wed Apr 08 18:47:...|\n|/gdnhealthcare|Most viewed this ...|Wed Apr 08 17:49:...|\n|/gdnhealthcare|How can technolog...|Wed Apr 08 17:41:...|\n|/gdnhealthcare|In case you misse...|Wed Apr 08 16:39:...|\n|/gdnhealthcare|\"India is a stark...|Wed Apr 08 13:45:...|\n|/gdnhealthcare|Popular on the ne...|Wed Apr 08 12:43:...|\n|/gdnhealthcare|Are there any top...|Wed Apr 08 11:52:...|\n|/gdnhealthcare|Lots of debate go...|Wed Apr 08 10:40:...|\n|/gdnhealthcare|Our weekly newsle...|Wed Apr 08 10:06:...|\n|/gdnhealthcare|New today: I've s...|Wed Apr 08 08:39:...|\n|/gdnhealthcare|More than 140 top...|Wed Apr 08 08:32:...|\n|/gdnhealthcare|Don't miss: Techn...|Tue Apr 07 19:53:...|\n|/gdnhealthcare|Most read today: ...|Tue Apr 07 18:50:...|\n|/gdnhealthcare|New today: I've s...|Tue Apr 07 17:49:...|\n|/gdnhealthcare|\"I hope that 2015...|Tue Apr 07 16:48:...|\n|/gdnhealthcare|Foreign born Brit...|Tue Apr 07 13:45:...|\n|/gdnhealthcare|Why the UK public...|Tue Apr 07 13:16:...|\n|/gdnhealthcare|Our free newslett...|Tue Apr 07 11:59:...|\n|/gdnhealthcare|RT @_louisethomas...|Tue Apr 07 11:50:...|\n+--------------+--------------------+--------------------+\nonly showing top 20 rows\n\n"}], "execution_count": 73}, {"source": "import nltk\nimport re\n# define a function for tokenizing and stemming words\ndef stemAndtokenize(text_final): \n    nltk.download('punkt')\n    p = nltk.PorterStemmer()\n    # stemming words \n    tokens = p.stem(text_final)\n    # tokenizing words:\n    tokens = nltk.word_tokenize(text_final)\n    tokens2 = [re.sub('[^A-Za-z0-9]+', '',x) for x in tokens]\n    #tokens3 = tokens2.map(lambda x: (x[0][1:], [word for word in x[1] if len(word)>0]))\n    \n    return tokens2\n\ndef refiner(text):\n    tokens = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '',text)\n    from nltk.corpus import stopwords\n    nltk.data.path.append(\"/nltk_data\")\n    stop = stopwords.words('english')\n    avoid = ['What', 'I', 'what', 'A', 'US', 'How', 'may', 'says', 'Most', 'most', 'Health', 'health', 'healthcare', 'The', 'HealthTalk', \n                    'healthtalk', 'To', 'In', 'to', 'in', 'us', 'New', 'Well', 'May', 'Why', 'For', 'Is', 'new', 'E', 'EverydayHealth', 'RT', 'NHS']\n    stop.extend(avoid)\n    pattern = re.compile(r'\\b(' + r'|'.join(stop) + r')\\b\\s*')\n    text_final = pattern.sub('', tokens)\n    \n    print(text_final)\n    \n    return text_final", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 57}, {"source": "from operator import add\n\n# we want to visualize word by counting them by months,years to see which word has been used the most for a certain month,year.\n\n# Create key-values pair with text on x[1] and dates on x[2]\nrdd1 = full_rdd.map(lambda x: (x[2], (x[1])))\n# take values\nrdd2 = rdd1.values()\n# call text refiner, remove URLs and stopwords \nrdd3 = rdd2.map(refiner)\n# call tokenizer, tokenize and keep only words, removing hashtags, punctuation, lowerings and removing plurals....\nrdd4 = rdd3.map(stemAndtokenize)\n# Zipp keys and values together... \nrdd6 = rdd1.keys().zip(rdd4)\n# Setting date to a readable format and delete whitespaces\nrdd7 = rdd6.map(lambda x: (x[0][4:7]+(x[0][-4:]), [s for s in x[1] if len(s)>0]))\n# Erease eventual white spaces...\nrdd_final = rdd7.map(lambda x: (x[0][0:], [word for word in x[1] if len(word)>0]))\n\ndef countbyMonths(rdd):\n    # Collect into array all the months/years\n    months = rdd.map(lambda x: x[0]).distinct().collect()\n    # Inizialize an Empty RDD\n    outputs = sc.parallelize(str(\"\"))\n        # Iterate through all the months, this could take some times... go have a tea...\n    for i in months:\n        # Take all the observations with that particular months and yeara(example: all the text with \"oct2014\")\n        t1 = rdd.filter(lambda x: x[0]==str(i))\n        # flatMap to produce touple with key and value (word,1) of the sorted RDD\n        t2 = t1.flatMap(lambda x: (x[1])).map(lambda x: (x,1))\n        # count by key, to obtain the number of words for that particular month/years combination\n        t3 = t2.reduceByKey(add)\n        # Order by count\n        t4 = t3.sortBy(lambda x: -x[1])\n        # attach months/years key to word,count touple.\n        t5 = t4.map(lambda x: (str(i) ,x[0], x[1]))\n        # insert the result in the empty rdd\n        outputs = outputs.union(t5)\n        \n    return outputs\n          \n# Uncomment to re-run the long code.\nrdd_df = countbyMonths(rdd_final)\n# Create schema for the dataframe\ndf = rdd_df.map(lambda x: Row(month=x[0], word=x[1], count=x[2]))\n# Create dataframe using the schema.\ndf = sqlContext.createDataFrame(rdd_df, ['month','word','count'])\n# Sort descending order\ndf = df.sort(desc(\"count\"))\n# Print dataframe...\ndf.show()\n", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------+-------+-----+\n|  month|   word|count|\n+-------+-------+-----+\n|Oct2014|  Ebola| 1678|\n|Aug2014|  Ebola|  602|\n|Sep2014|  Ebola|  481|\n|Nov2014|  Ebola|  425|\n|Dec2014|  Ebola|  304|\n|Oct2014|     US|  283|\n|Jan2015|    amp|  193|\n|Jan2015|  Ebola|  169|\n|Oct2014| Dallas|  161|\n|Sep2014|     US|  145|\n|Oct2014|patient|  144|\n|Mar2015|  Ebola|  138|\n|Oct2014| Africa|  126|\n|Jul2014|  Ebola|  123|\n|Jan2013|  foods|  123|\n|Feb2013|  foods|  120|\n|Oct2014|  Texas|  117|\n|Oct2014|    CDC|  113|\n|Aug2014|   drug|  112|\n|Aug2014|     US|  109|\n+-------+-------+-----+\nonly showing top 20 rows\n\n"}], "execution_count": 6}, {"source": "# We want to predict the author of the tweet using a multiclass logistic regression, we need first to work on the RDD, deleting stopwords, tokenizing,\n# and refining.\n\n# Initialize dataset:\n# You can pass inside the initializer an RDD and it will process it by tokenizing, stopword removing, stemming and returning a processed RDD\ndef initialize(rdd):\n# Define the RDD to process, define a tuple of keys and values, where keys are x[1] and they will be our label, and word are the second part of our tuple.  \n    rdd = rdd.map(lambda x: (x[0], (x[1])))\n    # separate the values from the keys, to work on just on the text \n    values = rdd.values()\n    # call the refiner function on the rdd\n    rdd_refined = values.map(refiner)\n    # call the stemmer and tokenizer function on rdd\n    rdd_tokenized = rdd_refined.map(stemAndtokenize)\n    # re-zip keys and values together.\n    zipped_rdd = rdd.keys().zip(rdd_tokenized)\n    # remove empty spaces from RDD due to the refiner and tokenizer actions.\n    model_rdd = zipped_rdd.map(lambda x: (x[0][1:], [word for word in x[1] if len(word)>0]))\n    \n    return model_rdd\n    \n    \n\n# This function is meant to convert the RDD in a dataframe processable by pyspark.ml library --- we want to predict the author of the tweet, pyspark.ml \n# need a dataframe object so we need to convert before proceed to the machine learning part.\ndef to_indexed_dataframe(rdd):\n    # define column name for the dataframe\n    columns = ['accounts', 'text']\n    # transform the rdd into a df with the \n    df = rdd.toDF(columns)\n    # transform the tweet author name into a numerical index processable for multiclass classification.\n    indexer = StringIndexer(inputCol=\"accounts\", outputCol=\"label\")\n    indexed_df = indexer.fit(df).transform(df)\n    \n    return indexed_df", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 8}, {"source": "from pyspark.ml.feature import CountVectorizer\nfrom pyspark.ml.clustering import LDA\n\ndef makeLda(pippo):\n    # create vector object\n    cv = CountVectorizer(inputCol=\"text\", outputCol=\"vectors\")\n\n    #map action. model fitted on the single workers\n    lda_an=cv.fit(pippo)\n    vocab= lda_an.vocabulary\n\n    #reduce action, all the map fit actions are reduced/put togheter, and then showed\n    #lda_an.transform(lda_df).show()\n    lda_trans= lda_an.transform(lda_df)\n\n    #specify parameters of LDA; k=2 number of topics. \n    lda = LDA(k=10, seed=1, optimizer=\"em\", featuresCol='vectors')\n\n    #run LDA on \n    model = lda.fit(lda_trans)\n    return model, vocab    ", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 52}, {"source": "full_rdd= LockAndLoad('Health-Tweets')\n#start from rdd structure\nlda_rdd= initialize(full_rdd)\n#print(lda_rdd.take(3))\ndf= to_indexed_dataframe(lda_rdd)\n\n#call function pippo on df to create vocabulary and fit lds\nmodelLda, vocab = makeLda(df)\n\n#modelLda.describeTopics().show()\n\n#code from https://datascience.stackexchange.com/questions/20061/words-from-lda-output-pyspark-ml\ntopics= modelLda.describeTopics()\ntopics_rdd = topics.rdd\ntopics_words = topics_rdd\\\n       .map(lambda row: row['termIndices'])\\\n       .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n       .collect()\n\nfor idx, topic in enumerate(topics_words):\n    print(\"topic: \", idx)\n    print(\"----------\")\n\n    for word in topic:\n        print(word)\n        print(\"----------\")", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "[PosixPath('Health-Tweets/gdnhealthcare.txt'), PosixPath('Health-Tweets/NBChealth.txt'), PosixPath('Health-Tweets/usnewshealth.txt'), PosixPath('Health-Tweets/KaiserHealthNews.txt'), PosixPath('Health-Tweets/reuters_health.txt'), PosixPath('Health-Tweets/bbchealth.txt'), PosixPath('Health-Tweets/msnhealthnews.txt'), PosixPath('Health-Tweets/foxnewshealth.txt'), PosixPath('Health-Tweets/cbchealth.txt'), PosixPath('Health-Tweets/cnnhealth.txt'), PosixPath('Health-Tweets/latimeshealth.txt'), PosixPath('Health-Tweets/nytimeshealth.txt'), PosixPath('Health-Tweets/nprhealth.txt'), PosixPath('Health-Tweets/wsjhealth.txt'), PosixPath('Health-Tweets/everydayhealth.txt')]\nlen(rddarray) 15\nlen(filenames) 15\n+-----+--------------------+--------------------+\n|topic|         termIndices|         termWeights|\n+-----+--------------------+--------------------+\n|    0|[0, 1, 2, 4, 5, 3...|[0.00990515200241...|\n|    1|[0, 1, 2, 4, 3, 5...|[0.00973377960392...|\n|    2|[0, 1, 2, 4, 3, 5...|[0.00965972342403...|\n|    3|[0, 1, 2, 4, 5, 3...|[0.00978887640959...|\n|    4|[0, 1, 2, 5, 3, 4...|[0.00956313795797...|\n|    5|[0, 1, 2, 3, 5, 4...|[0.00994111592303...|\n|    6|[0, 1, 2, 5, 4, 3...|[0.00968635684640...|\n|    7|[0, 1, 4, 2, 3, 5...|[0.00986847224867...|\n|    8|[0, 1, 2, 3, 4, 5...|[0.00998342423381...|\n|    9|[0, 1, 2, 5, 3, 4...|[0.00978885213057...|\n+-----+--------------------+--------------------+\n\ntopic:  0\n----------\nEbola\n----------\nUS\n----------\nstudy\n----------\nStudy\n----------\ncancer\n----------\namp\n----------\nvia\n----------\ncare\n----------\nWith\n----------\ndrug\n----------\ntopic:  1\n----------\nEbola\n----------\nUS\n----------\nstudy\n----------\nStudy\n----------\namp\n----------\ncancer\n----------\nWith\n----------\nvia\n----------\ncare\n----------\ndrug\n----------\ntopic:  2\n----------\nEbola\n----------\nUS\n----------\nstudy\n----------\nStudy\n----------\namp\n----------\ncancer\n----------\nvia\n----------\nMore\n----------\nWith\n----------\ncare\n----------\ntopic:  3\n----------\nEbola\n----------\nUS\n----------\nstudy\n----------\nStudy\n----------\ncancer\n----------\namp\n----------\nWith\n----------\nvia\n----------\ndrug\n----------\nMore\n----------\ntopic:  4\n----------\nEbola\n----------\nUS\n----------\nstudy\n----------\ncancer\n----------\namp\n----------\nStudy\n----------\nvia\n----------\nWith\n----------\ndrug\n----------\ncare\n----------\ntopic:  5\n----------\nEbola\n----------\nUS\n----------\nstudy\n----------\namp\n----------\ncancer\n----------\nStudy\n----------\ndrug\n----------\nvia\n----------\nMore\n----------\nWith\n----------\ntopic:  6\n----------\nEbola\n----------\nUS\n----------\nstudy\n----------\ncancer\n----------\nStudy\n----------\namp\n----------\nvia\n----------\nWith\n----------\ndrug\n----------\nMore\n----------\ntopic:  7\n----------\nEbola\n----------\nUS\n----------\nStudy\n----------\nstudy\n----------\namp\n----------\ncancer\n----------\nvia\n----------\ndrug\n----------\nWith\n----------\nMore\n----------\ntopic:  8\n----------\nEbola\n----------\nUS\n----------\nstudy\n----------\namp\n----------\nStudy\n----------\ncancer\n----------\nWith\n----------\nvia\n----------\ndrug\n----------\nMore\n----------\ntopic:  9\n----------\nEbola\n----------\nUS\n----------\nstudy\n----------\ncancer\n----------\namp\n----------\nStudy\n----------\nvia\n----------\nWith\n----------\nMore\n----------\ndrug\n----------\n"}], "execution_count": 74}, {"source": "                \n                    #### LOADING PACKAGES ####\n    \nfrom pyspark.ml.feature import HashingTF, IDF, StringIndexer\nfrom pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import Pipeline\n    \n\n# we want a function that tests all the different models and return the best one:\ndef trainbest(df, weights_array):\n    # Initialize Multinomial Logistic Regression:\n    lr = LogisticRegression(maxIter=5, family=\"multinomial\")\n    # define the hashing function:\n    hashingTF = HashingTF(inputCol=\"text\", outputCol=\"features\")\n\n    # define the schedule for the ParamGrid search, paramgrid will iterate through different parameters combinations \n    # starting from the hashing function(hashingTF), modifying vector size(numFeatures) and then running the logistic regression(lr) \n    # with different parameters configuration(regParam & elastNetParam).\n    schedule = Pipeline(stages=[hashingTF, lr])\n\n    # build up the grid for grid search\n    paramGrid = ParamGridBuilder()\\\n        .addGrid(hashingTF.numFeatures, [5, 8, 6]) \\\n        .addGrid(lr.regParam, [0.1, 0.03, 0.01]) \\\n        .addGrid(lr.elasticNetParam, [0.0, 0.3, 1.0])\\\n        .build()\n\n    # define the evaluator for multiclass classification, this is needed to calculate the accuracy for each iteration and choose the best model, \n    # this is also needed to calculate the accuracy with the test sample.\n    MCevaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n\n    # this initialize the grid search, define the schedule, the grid and the evaluator, with the size \n    # for train and validation set fixed to 0.8 of the original train set.    \n    tvs = TrainValidationSplit(estimator=schedule,\n                               estimatorParamMaps=paramGrid,\n                               evaluator=MCevaluator,\n                               # 80% of the data will be used for training, 20% for validation.\n                               trainRatio=0.8)\n    # initialize an empty array to store test accuracy\n    test_acc = []\n    # initialize an empty array to store models. \n    models = []\n    # iterate throught the weight array to split dataset in different size.\n    for tr_weight in weights_array:\n        # get test size\n        te_weight = 1-tr_weight\n        # split data with the actual loop stage value\n        train, test = df.randomSplit([tr_weight, te_weight])\n        # fit model, the model that perform best in the gridsearch with the validation set is memorized\n        model = tvs.fit(train)\n        # make predictions on test data\n        predictions = model.transform(test)\n        # calculate accuracy\n        accuracy = MCevaluator.evaluate(predictions)\n        accuracy = 1.0 - accuracy \n        # store accuray and the model in the arrays for comparison\n        test_acc.append(accuracy)\n        models.append(model)\n        print(\"... best model for this iteration: Test Error = %g \" % (accuracy))\n        # repeat...for all the train,test size array...\n    # once the loop is over...    \n    # check which one has the best accuracy and report the number\n    best_acc = max(test_acc)\n    #  the best trained model is the one which have obtained the best accuracy for that particular train-test split.\n    best_model = models[test_acc.index(max(test_acc))]\n    \n    return best_acc, best_model", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 10}, {"source": "# Initialize RDD #\nrdd = initialize(full_rdd)\n# Transform to dataframe #\n# print(rdd.take(1)) <<<< FOR DEBUG\ndf = to_indexed_dataframe(rdd)\n# define a set of weights to split the dataset #\nsplit_weights = [0.8]\n# call the function to train and obtain the best trained model and its accuracy on test data. #\nacc, model = trainbest(df, split_weights)\n\nprint(\" Overall best model has Test Error = %g \" % acc)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "... best model for this iteration: Test Error = 0.870052 \n Overall best model has Test Error = 0.870052 \n"}], "execution_count": 11}], "metadata": {"kernelspec": {"display_name": "Python 3.5 with Spark 2.1", "name": "python3-spark21", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.5.4", "name": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4}